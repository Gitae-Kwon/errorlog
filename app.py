# app_kpi.py
# Streamlit KPI ì¹´ë“œí˜• ëŒ€ì‹œë³´ë“œ + í•„í„° + ì—…ë¡œë“œ/ì‚­ì œ (MySQL RDS)
# ---------------------------------------------------------------
# Secrets(.streamlit/secrets.toml ë˜ëŠ” Cloud Secrets) ì˜ˆì‹œ:
# [db]                               # ë˜ëŠ” [DB] ë¡œ ì˜¬ë ¤ë„ ë¨
# HOST = "my-db-7.c7s06yiach58.ap-northeast-2.rds.amazonaws.com"  # DB_HOST
# PORT = 3306                                                             # DB_PORT
# USER = "admin"                                                          # DB_USER
# PASSWORD = "qwer4321!!K"                                               # DB_PASSWORD
# NAME = "mydata"                                                         # DB_NAME
#
# requirements.txt (í•µì‹¬)
# streamlit>=1.36
# pandas>=2.2
# SQLAlchemy>=2.0
# pymysql>=1.1
# altair>=5.3
# python-dateutil>=2.9
# cryptography>=42.0
# ---------------------------------------------------------------

import os
import pandas as pd
import altair as alt
import streamlit as st
from datetime import datetime, timedelta
from sqlalchemy import create_engine, text

st.set_page_config(page_title="ì¥ì•  í˜„í™© ëŒ€ì‹œë³´ë“œ", page_icon="ğŸ“Š", layout="wide")
st.title("ğŸ“Š ì¥ì•  í˜„í™© ëŒ€ì‹œë³´ë“œ")
st.caption("KPI ì¹´ë“œ Â· í•„í„° Â· ì—…ë¡œë“œ/ì‚­ì œ ê´€ë¦¬")

# ---------------------------
# DB ì—°ê²° (Secrets ì½ê¸° + SSL ê°•ì œ)
# ---------------------------
@st.cache_resource(show_spinner=False)
def get_engine():
    s = st.secrets
    cfg = {}
    if "db" in s:
        cfg = s["db"]
        host = cfg.get("HOST") or cfg.get("host")
        port = int(cfg.get("PORT") or cfg.get("port") or 3306)
        user = cfg.get("USER") or cfg.get("user")
        pw   = cfg.get("PASSWORD") or cfg.get("password")
        name = cfg.get("NAME") or cfg.get("name")
    elif "DB" in s:
        cfg = s["DB"]
        host = cfg.get("DB_HOST") or cfg.get("HOST")
        port = int(cfg.get("DB_PORT") or cfg.get("PORT") or 3306)
        user = cfg.get("DB_USER") or cfg.get("USER")
        pw   = cfg.get("DB_PASSWORD") or cfg.get("PASSWORD")
        name = cfg.get("DB_NAME") or cfg.get("NAME")
    else:
        host = os.getenv("DB_HOST"); port = int(os.getenv("DB_PORT") or 3306)
        user = os.getenv("DB_USER"); pw = os.getenv("DB_PASSWORD"); name = os.getenv("DB_NAME")

    if not all([host, user, pw, name]):
        st.error("DB secretsê°€ ì—†ìŠµë‹ˆë‹¤. [db] ë˜ëŠ” [DB] ì„¹ì…˜ìœ¼ë¡œ HOST/PORT/USER/PASSWORD/NAMEë¥¼ ë“±ë¡í•˜ì„¸ìš”.")
        st.stop()

    # SQLAlchemy URL
    url = f"mysql+pymysql://{user}:{pw}@{host}:{port}/{name}?charset=utf8mb4"

    # í•µì‹¬: MySQL 8 (caching_sha2_password)ì—ì„œ TLS ì—†ìœ¼ë©´ ì¸ì¦ ì‹¤íŒ¨ â†’ ssl ê°•ì œ
    connect_args = {"ssl": {"ssl": True}}

    try:
        eng = create_engine(url, pool_pre_ping=True, connect_args=connect_args)
        # ë¯¸ë¦¬ ì—°ê²° í™•ì¸
        with eng.connect() as conn:
            conn.execute(text("SELECT 1"))
        return eng
    except Exception as e:
        st.error(f"DB ì—°ê²° ì‹¤íŒ¨: {e}")
        st.stop()

engine = get_engine()

# ---------------------------
# ìœ í‹¸ / ì¿¼ë¦¬
# ---------------------------
@st.cache_data(ttl=180, show_spinner=False)
def get_distinct_values():
    with engine.connect() as conn:
        platforms = pd.read_sql(text(
            "SELECT DISTINCT platform FROM incidents "
            "WHERE platform IS NOT NULL AND platform<>'' ORDER BY platform"
        ), conn)["platform"].tolist()
        locales = pd.read_sql(text(
            "SELECT DISTINCT locale FROM incidents "
            "WHERE locale IS NOT NULL AND locale<>'' ORDER BY locale"
        ), conn)["locale"].tolist()
        cats = pd.read_sql(text(
            "SELECT DISTINCT category FROM incidents "
            "WHERE category IS NOT NULL AND category<>'' ORDER BY category"
        ), conn)["category"].tolist()
    return platforms, locales, cats

PLATFORMS, LOCALES, CATEGORIES = get_distinct_values()

# ---------------------------
# ì‚¬ì´ë“œë°” í•„í„°
# ---------------------------
with st.sidebar:
    st.header("í•„í„°")
    today = datetime.now().date()
    default_from = today - timedelta(days=30)
    date_from, date_to = st.date_input("ê¸°ê°„(started_at)", value=(default_from, today))
    if isinstance(date_from, tuple):  # ì•ˆì „ì¥ì¹˜
        date_from, date_to = date_from

    sel_platforms = st.multiselect("í”Œë«í¼", options=PLATFORMS)
    sel_locales   = st.multiselect("ë¡œì¼€ì¼", options=LOCALES)
    sel_categories= st.multiselect("ì¹´í…Œê³ ë¦¬", options=CATEGORIES)
    keyword = st.text_input("í‚¤ì›Œë“œ(ë‚´ìš©/ì›ì¸/ëŒ€ì‘/ë¹„ê³ )")
    limit = st.number_input("ëª©ë¡ í–‰ìˆ˜", min_value=50, max_value=5000, value=500, step=50)

# WHERE êµ¬ì„±
params = {"date_from": datetime.combine(date_from, datetime.min.time()),
          "date_to":   datetime.combine(date_to,   datetime.max.time())}
where = ["i.started_at BETWEEN :date_from AND :date_to"]
if sel_platforms:
    where.append("i.platform IN :platforms");   params["platforms"]  = tuple(sel_platforms)
if sel_locales:
    where.append("i.locale IN :locales");       params["locales"]    = tuple(sel_locales)
if sel_categories:
    where.append("i.category IN :categories");  params["categories"] = tuple(sel_categories)
if keyword.strip():
    where.append("(i.description LIKE :kw OR i.cause LIKE :kw OR i.response LIKE :kw OR i.note LIKE :kw)")
    params["kw"] = f"%{keyword.strip()}%"
where_sql = " AND ".join(where)

@st.cache_data(ttl=90, show_spinner=False)
def fetch_kpis(where_sql: str, params: dict):
    with engine.connect() as conn:
        total = pd.read_sql(text(f"SELECT COUNT(*) cnt FROM incidents i WHERE {where_sql}"), conn, params=params)["cnt"].iloc[0]
        tparams = dict(params)
        tparams["date_from"] = datetime.combine(datetime.now().date(), datetime.min.time())
        tparams["date_to"]   = datetime.combine(datetime.now().date(), datetime.max.time())
        today_cnt = pd.read_sql(text(f"SELECT COUNT(*) cnt FROM incidents i WHERE {where_sql}"), conn, params=tparams)["cnt"].iloc[0]
        top_cat_df = pd.read_sql(text(
            f"SELECT i.category, COUNT(*) cnt FROM incidents i WHERE {where_sql} GROUP BY i.category ORDER BY cnt DESC LIMIT 1"
        ), conn, params=params)
        top_cat = (top_cat_df["category"].iloc[0], int(top_cat_df["cnt"].iloc[0])) if not top_cat_df.empty else ("-", 0)
        plat_df = pd.read_sql(text(
            f"SELECT i.platform, COUNT(*) cnt FROM incidents i WHERE {where_sql} GROUP BY i.platform ORDER BY cnt DESC"
        ), conn, params=params)
    return total, today_cnt, top_cat, plat_df

@st.cache_data(ttl=90, show_spinner=False)
def fetch_timeseries(where_sql: str, params: dict) -> pd.DataFrame:
    with engine.connect() as conn:
        return pd.read_sql(text(
            f"SELECT DATE(i.started_at) d, COUNT(*) cnt FROM incidents i WHERE {where_sql} GROUP BY DATE(i.started_at) ORDER BY d"
        ), conn, params=params)

@st.cache_data(ttl=90, show_spinner=False)
def fetch_list(where_sql: str, params: dict, limit: int) -> pd.DataFrame:
    q = text(f"""
        SELECT i.id, i.started_at, i.ended_at, i.duration, i.platform, i.locale, i.inquiry_count,
               i.category, i.description, i.cause, i.response, i.note, i.created_at, i.updated_at
        FROM incidents i
        WHERE {where_sql}
        ORDER BY i.started_at DESC
        LIMIT :limit
    """)
    p = dict(params); p["limit"] = int(limit)
    with engine.connect() as conn:
        df = pd.read_sql(q, conn, params=p)
    if not df.empty:
        df["started_at"] = pd.to_datetime(df["started_at"]).dt.strftime("%Y-%m-%d %H:%M")
        df["ended_at"]   = df["ended_at"].apply(lambda x: "" if pd.isna(x) else pd.to_datetime(x).strftime("%Y-%m-%d %H:%M"))
    return df

# ---------------------------
# KPI ì¹´ë“œ
# ---------------------------
try:
    total, today_cnt, (top_cat_name, top_cat_cnt), plat_df = fetch_kpis(where_sql, params)
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("ì´ ê±´ìˆ˜", f"{total:,}")
    c2.metric("ì˜¤ëŠ˜ ê±´ìˆ˜", f"{today_cnt:,}")
    c3.metric("ìµœë‹¤ ì¹´í…Œê³ ë¦¬", top_cat_name, delta=f"{top_cat_cnt:,}ê±´")
    with c4:
        st.write("í”Œë«í¼ë³„")
        if not plat_df.empty:
            st.altair_chart(
                alt.Chart(plat_df).mark_bar().encode(
                    x=alt.X('platform:N', sort='-y'), y='cnt:Q', tooltip=['platform','cnt']
                ),
                use_container_width=True
            )
        else:
            st.info("ë°ì´í„° ì—†ìŒ")
except Exception as e:
    st.warning(f"KPI ë¡œë”© ì˜¤ë¥˜: {e}")

# ---------------------------
# ì¼ë³„ ì¶”ì´
# ---------------------------
st.subheader("ğŸ“ˆ ì¼ë³„ ë°œìƒ ì¶”ì´")
ts_df = fetch_timeseries(where_sql, params)
if ts_df.empty:
    st.info("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.altair_chart(
        alt.Chart(ts_df).mark_line(point=True).encode(x='d:T', y='cnt:Q', tooltip=['d:T','cnt:Q']),
        use_container_width=True
    )

# ---------------------------
# ëª©ë¡
# ---------------------------
st.subheader("ğŸ“„ ì‚¬ê±´ ëª©ë¡")
list_df = fetch_list(where_sql, params, int(limit))
if list_df.empty:
    st.info("ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.dataframe(list_df, use_container_width=True, height=420)

# ---------------------------
# ê´€ë¦¬: ì„ íƒ ì‚­ì œ
# ---------------------------
with st.expander("ğŸ—‘ ê´€ë¦¬: IDë¡œ ì„ íƒ ì‚­ì œ"):
    ids = st.text_input("ì‚­ì œí•  IDë“¤(ì‰¼í‘œë¡œ êµ¬ë¶„)", placeholder="ì˜ˆ: 101,102,120")
    if st.button("ì‚­ì œ ì‹¤í–‰", type="primary"):
        try:
            id_list = [int(x.strip()) for x in ids.split(',') if x.strip()]
            if not id_list:
                st.warning("IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            else:
                with engine.begin() as conn:
                    conn.execute(text("DELETE FROM incidents WHERE id IN :ids"), {"ids": tuple(id_list)})
                st.success(f"ì‚­ì œ ì™„ë£Œ: {len(id_list)}ê±´")
                st.cache_data.clear()
        except Exception as e:
            st.error(f"ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")

# ---------------------------
# ê´€ë¦¬: ì—…ë¡œë“œ (CSV/ì—‘ì…€)
# ---------------------------
with st.expander("â¬†ï¸ ê´€ë¦¬: ì—…ë¡œë“œ (CSV/XLSX)"):
    st.caption("ê°€ëŠ¥í•œ ì»¬ëŸ¼: started_at, ended_at, duration, platform, locale, inquiry_count, category, description, cause, response, note")
    file = st.file_uploader("íŒŒì¼ ì„ íƒ", type=["csv", "xlsx", "xls"])

    def normalize_df(df: pd.DataFrame) -> pd.DataFrame:
        # ì»¬ëŸ¼ ì •ê·œí™”
        df.columns = [str(c).strip().lower() for c in df.columns]
        # ë‚ ì§œ íŒŒì‹±
        for dt in ["started_at", "ended_at"]:
            if dt in df.columns:
                df[dt] = pd.to_datetime(df[dt], errors="coerce")
        if "inquiry_count" in df.columns:
            df["inquiry_count"] = pd.to_numeric(df["inquiry_count"], errors="coerce")
        # ëˆ„ë½ ì±„ìš°ê¸°
        for col in ["ended_at","duration","platform","locale","inquiry_count","cause","response","note"]:
            if col not in df.columns:
                df[col] = None
        # í•„ìˆ˜ í™•ì¸
        required = ["started_at", "category", "description"]
        missing = [c for c in required if c not in df.columns]
        if missing:
            raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")
        return df[["started_at","ended_at","duration","platform","locale","inquiry_count",
                   "category","description","cause","response","note"]]

    if file is not None:
        try:
            up = pd.read_csv(file) if file.name.lower().endswith(".csv") else pd.read_excel(file)
            up = normalize_df(up)
            with engine.begin() as conn:
                up.to_sql("incidents", conn, if_exists="append", index=False)
            st.success(f"ì—…ë¡œë“œ ì™„ë£Œ: {len(up)}ê±´")
            st.cache_data.clear()
        except Exception as e:
            st.error(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
