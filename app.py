# app_kpi.py
# Streamlit KPI ì¹´ë“œí˜• ëŒ€ì‹œë³´ë“œ + í•„í„° + ê´€ë¦¬(ì—…ë¡œë“œ/ì‚­ì œ)
# -------------------------------------------------------------
# 1) .streamlit/secrets.toml
# [db]
# HOST = "my-db-7.c7s06yiach58.ap-northeast-2.rds.amazonaws.com"
# PORT = 3306
# USER = "admin"
# PASSWORD = "qwer4321!!K"
# NAME = "mydata"
# -------------------------------------------------------------

import pandas as pd
import altair as alt
import streamlit as st
from datetime import datetime, timedelta
from sqlalchemy import create_engine, text

st.set_page_config(page_title="Service Failures Dashboard", page_icon="ğŸ“Š", layout="wide")
st.title("ğŸ“Š ì¥ì•  í˜„í™© ëŒ€ì‹œë³´ë“œ")
st.caption("KPI ì¹´ë“œ + í•„í„° + ì—…ë¡œë“œ/ì‚­ì œ ê´€ë¦¬")

# ---------------------------
# DB ì—°ê²°
# ---------------------------
@st.cache_resource(show_spinner=False)
def get_engine():
    cfg = st.secrets["db"]
    url = (
        f"mysql+pymysql://{cfg['USER']}:{cfg['PASSWORD']}@{cfg['HOST']}:{int(cfg['PORT'])}/{cfg['NAME']}?charset=utf8mb4"
    )
    return create_engine(url, pool_pre_ping=True)

engine = get_engine()

# ---------------------------
# ê³µí†µ ì¿¼ë¦¬
# ---------------------------
@st.cache_data(ttl=120, show_spinner=False)
def get_distinct_values():
    sql = text("""
        SELECT DISTINCT platform FROM incidents WHERE platform IS NOT NULL AND platform<>'' ORDER BY platform;
        SELECT DISTINCT locale   FROM incidents WHERE locale   IS NOT NULL AND locale<>''   ORDER BY locale;
        SELECT DISTINCT category FROM incidents WHERE category IS NOT NULL AND category<>'' ORDER BY category;
    """)
    # MySQLì€ ë©€í‹°ì¿¼ë¦¬ë¥¼ í•œë²ˆì— ëª» ì½ì„ ìˆ˜ ìˆì–´ ìˆœì°¨ë¡œ ì‹¤í–‰
    with engine.connect() as conn:
        platforms = pd.read_sql(text("SELECT DISTINCT platform FROM incidents WHERE platform IS NOT NULL AND platform<>'' ORDER BY platform"), conn)["platform"].tolist()
        locales   = pd.read_sql(text("SELECT DISTINCT locale   FROM incidents WHERE locale   IS NOT NULL AND locale<>''   ORDER BY locale"), conn)["locale"].tolist()
        cats      = pd.read_sql(text("SELECT DISTINCT category FROM incidents WHERE category IS NOT NULL AND category<>'' ORDER BY category"), conn)["category"].tolist()
    return platforms, locales, cats

PLATFORMS, LOCALES, CATEGORIES = get_distinct_values()

# ---------------------------
# ì‚¬ì´ë“œë°” í•„í„°
# ---------------------------
with st.sidebar:
    st.header("í•„í„°")
    today = datetime.now().date()
    default_from = today - timedelta(days=30)
    date_from, date_to = st.date_input("ê¸°ê°„(started_at ê¸°ì¤€)", value=(default_from, today))
    if isinstance(date_from, tuple):
        date_from, date_to = date_from

    sel_platforms = st.multiselect("í”Œë«í¼", options=PLATFORMS)
    sel_locales   = st.multiselect("ë¡œì¼€ì¼", options=LOCALES)
    sel_categories= st.multiselect("ì¹´í…Œê³ ë¦¬", options=CATEGORIES)
    keyword = st.text_input("í‚¤ì›Œë“œ(ë‚´ìš©/ì›ì¸/ëŒ€ì‘/ë¹„ê³ )")
    limit = st.number_input("ëª©ë¡ í–‰ìˆ˜", min_value=50, max_value=5000, value=500, step=50)

params = {
    "date_from": datetime.combine(date_from, datetime.min.time()),
    "date_to":   datetime.combine(date_to,   datetime.max.time()),
}
where = ["i.started_at BETWEEN :date_from AND :date_to"]
if sel_platforms:
    where.append("i.platform IN :platforms")
    params["platforms"] = tuple(sel_platforms)
if sel_locales:
    where.append("i.locale IN :locales")
    params["locales"] = tuple(sel_locales)
if sel_categories:
    where.append("i.category IN :categories")
    params["categories"] = tuple(sel_categories)
if keyword.strip():
    where.append("(i.description LIKE :kw OR i.cause LIKE :kw OR i.response LIKE :kw OR i.note LIKE :kw)")
    params["kw"] = f"%{keyword.strip()}%"
where_sql = " AND ".join(where)

# ---------------------------
# ë°ì´í„° ì¿¼ë¦¬ í•¨ìˆ˜
# ---------------------------
@st.cache_data(ttl=60, show_spinner=False)
def fetch_kpis(where_sql: str, params: dict):
    # ì „ì²´ ê±´ìˆ˜, ì˜¤ëŠ˜ ê±´ìˆ˜, ì¹´í…Œê³ ë¦¬ ìƒìœ„ 1, í”Œë«í¼ë³„ ê±´ìˆ˜
    with engine.connect() as conn:
        total = pd.read_sql(text(f"SELECT COUNT(*) AS cnt FROM incidents i WHERE {where_sql}"), conn, params=params)["cnt"].iloc[0]
        today_params = dict(params)
        today_params.update({
            "date_from": datetime.combine(datetime.now().date(), datetime.min.time()),
            "date_to": datetime.combine(datetime.now().date(), datetime.max.time())
        })
        today_cnt = pd.read_sql(text(f"SELECT COUNT(*) AS cnt FROM incidents i WHERE {where_sql}"), conn, params=today_params)["cnt"].iloc[0]
        top_cat_df = pd.read_sql(text(f"SELECT i.category, COUNT(*) AS cnt FROM incidents i WHERE {where_sql} GROUP BY i.category ORDER BY cnt DESC LIMIT 1"), conn, params=params)
        top_cat = (top_cat_df["category"].iloc[0], int(top_cat_df["cnt"].iloc[0])) if not top_cat_df.empty else ("-", 0)
        plat_df = pd.read_sql(text(f"SELECT i.platform, COUNT(*) AS cnt FROM incidents i WHERE {where_sql} GROUP BY i.platform ORDER BY cnt DESC"), conn, params=params)
    return total, today_cnt, top_cat, plat_df

@st.cache_data(ttl=60, show_spinner=False)
def fetch_timeseries(where_sql: str, params: dict) -> pd.DataFrame:
    sql = text(f"""
        SELECT DATE(i.started_at) AS d, COUNT(*) AS cnt
        FROM incidents i
        WHERE {where_sql}
        GROUP BY DATE(i.started_at)
        ORDER BY d
    """)
    with engine.connect() as conn:
        return pd.read_sql(sql, conn, params=params)

@st.cache_data(ttl=60, show_spinner=False)
def fetch_list(where_sql: str, params: dict, limit: int) -> pd.DataFrame:
    sql = text(f"""
        SELECT i.id, i.started_at, i.ended_at, i.duration, i.platform, i.locale, i.inquiry_count,
               i.category, i.description, i.cause, i.response, i.note, i.created_at, i.updated_at
        FROM incidents i
        WHERE {where_sql}
        ORDER BY i.started_at DESC
        LIMIT :limit
    """)
    p = dict(params)
    p["limit"] = int(limit)
    with engine.connect() as conn:
        df = pd.read_sql(sql, conn, params=p)
    if not df.empty:
        df["started_at"] = pd.to_datetime(df["started_at"]).dt.strftime("%Y-%m-%d %H:%M")
        df["ended_at"]   = df["ended_at"].apply(lambda x: "" if pd.isna(x) else pd.to_datetime(x).strftime("%Y-%m-%d %H:%M"))
    return df

# ---------------------------
# KPI ì¹´ë“œ ë Œë”ë§
# ---------------------------
col1, col2, col3, col4 = st.columns(4)
try:
    total, today_cnt, (top_cat_name, top_cat_cnt), plat_df = fetch_kpis(where_sql, params)
    col1.metric("ì´ ê±´ìˆ˜", f"{total:,}")
    col2.metric("ì˜¤ëŠ˜ ê±´ìˆ˜", f"{today_cnt:,}")
    col3.metric("ìµœë‹¤ ì¹´í…Œê³ ë¦¬", f"{top_cat_name}", delta=f"{top_cat_cnt:,}ê±´")
    # í”Œë«í¼ë³„ bar (ê°„ë‹¨ KPI)
    if not plat_df.empty:
        with col4:
            st.write("í”Œë«í¼ë³„")
            chart = alt.Chart(plat_df).mark_bar().encode(x=alt.X('platform:N', sort='-y'), y='cnt:Q', tooltip=['platform','cnt'])
            st.altair_chart(chart, use_container_width=True)
except Exception as e:
    st.warning(f"KPI ë¡œë”© ì˜¤ë¥˜: {e}")

# ---------------------------
# ì¼ë³„ ì¶”ì´ ì°¨íŠ¸
# ---------------------------
st.subheader("ğŸ“ˆ ì¼ë³„ ë°œìƒ ì¶”ì´")
ts_df = fetch_timeseries(where_sql, params)
if ts_df.empty:
    st.info("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    chart = alt.Chart(ts_df).mark_line(point=True).encode(x='d:T', y='cnt:Q', tooltip=['d:T','cnt:Q'])
    st.altair_chart(chart, use_container_width=True)

# ---------------------------
# ëª©ë¡ & ì„ íƒ ì‚­ì œ
# ---------------------------
st.subheader("ğŸ“„ ì‚¬ê±´ ëª©ë¡")
list_df = fetch_list(where_sql, params, int(limit))
if list_df.empty:
    st.info("ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.dataframe(list_df, use_container_width=True, height=420)
    # ì‚­ì œ ê¸°ëŠ¥
    with st.expander("ê´€ë¦¬: ì„ íƒ ì‚­ì œ"):
        ids = st.text_input("ì‚­ì œí•  ID ëª©ë¡(ì‰¼í‘œë¡œ êµ¬ë¶„)", placeholder="ì˜ˆ: 101,102,120")
        if st.button("ì‚­ì œ ì‹¤í–‰", type="primary"):
            try:
                id_list = [int(x.strip()) for x in ids.split(',') if x.strip()]
                if not id_list:
                    st.warning("IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                else:
                    q = text("DELETE FROM incidents WHERE id IN :ids")
                    with engine.begin() as conn:
                        conn.execute(q, {"ids": tuple(id_list)})
                    st.success(f"ì‚­ì œ ì™„ë£Œ: {len(id_list)}ê±´")
                    st.cache_data.clear()
            except Exception as e:
                st.error(f"ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")

# ---------------------------
# ê´€ë¦¬: ì—…ë¡œë“œ(ì—‘ì…€/CSV)
# ---------------------------
st.subheader("ğŸ›  ê´€ë¦¬: ì—…ë¡œë“œ")
with st.expander("ì—‘ì…€/CSV ì—…ë¡œë“œ"):
    file = st.file_uploader("íŒŒì¼ ì„ íƒ", type=["csv", "xlsx", "xls"])
    hint = st.caption("ì—´ ì´ë¦„ì€ ê°€ëŠ¥í•˜ë©´ ë‹¤ìŒì— ë§ì¶°ì£¼ì„¸ìš”: started_at, ended_at, duration, platform, locale, inquiry_count, category, description, cause, response, note")

    def normalize_df(df: pd.DataFrame) -> pd.DataFrame:
        # ì»¬ëŸ¼ ì†Œë¬¸ìí™” & ì–‘ë ê³µë°± ì œê±°
        df.columns = [str(c).strip().lower() for c in df.columns]
        # ë‚ ì§œ ì»¬ëŸ¼ íŒŒì‹±
        for col in ["started_at", "ended_at"]:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
        # íƒ€ì… ìºìŠ¤íŒ…
        if "inquiry_count" in df.columns:
            df["inquiry_count"] = pd.to_numeric(df["inquiry_count"], errors='coerce')
        # ëˆ„ë½ ì»¬ëŸ¼ ì±„ìš°ê¸°
        for col in ["duration","platform","locale","category","description","cause","response","note"]:
            if col not in df.columns:
                df[col] = None
        # í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬
        required = ["started_at", "category", "description"]
        missing = [c for c in required if c not in df.columns]
        if missing:
            raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")
        return df[["started_at","ended_at","duration","platform","locale","inquiry_count","category","description","cause","response","note"]]

    if file is not None:
        try:
            if file.name.lower().endswith('.csv'):
                df_up = pd.read_csv(file)
            else:
                df_up = pd.read_excel(file)
            df_up = normalize_df(df_up)
            # ì—…ë¡œë“œ
            with engine.begin() as conn:
                df_up.to_sql('incidents', conn, if_exists='append', index=False)
            st.success(f"ì—…ë¡œë“œ ì™„ë£Œ: {len(df_up)}ê±´")
            st.cache_data.clear()
        except Exception as e:
            st.error(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

st.markdown("\nâ€”\nğŸ’¡ KPI ì¹´ë“œëŠ” ì´ ê±´ìˆ˜/ì˜¤ëŠ˜ ê±´ìˆ˜/ìµœë‹¤ ì¹´í…Œê³ ë¦¬/í”Œë«í¼ë³„ ë¶„í¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. í•„ìš” ì‹œ Severity, ìƒíƒœ, ì„œë¹„ìŠ¤ ì˜ì—­ ì°¨ì›ì„ ì¶”ê°€í•´ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
